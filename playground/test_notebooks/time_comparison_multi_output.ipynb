{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparing a conventional and approximate Single-Output GP",
   "id": "5ee3db0d0d5e55c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T13:13:49.457824Z",
     "start_time": "2025-02-26T13:13:47.994132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "import famgpytorch\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on {torch.cuda.get_device_name(device) if device.type == 'cuda' else 'CPU'}.\")"
   ],
   "id": "374625387f378a3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on NVIDIA GeForce RTX 3080.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set up some very simple training data\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_1 &= \\sin(2\\pi x) + \\epsilon \\\\\n",
    "y_2 &= \\cos(2\\pi x) + \\epsilon \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(0, 0.04)\n",
    "\\end{align}\n",
    "$$\n",
    "With training and test examples regularly spaced points in [0,1]"
   ],
   "id": "e6c39af62626f8ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T13:13:49.620831Z",
     "start_time": "2025-02-26T13:13:49.459829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nb_training_points = 10000\n",
    "nb_test_points = 100\n",
    "\n",
    "train_x = torch.linspace(0, 1, nb_training_points, device=device)\n",
    "\n",
    "train_y = torch.stack([\n",
    "    torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size(), device=device) * math.sqrt(0.04),\n",
    "    torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size(), device=device) * math.sqrt(0.04),\n",
    "], -1)"
   ],
   "id": "370eae4905436054",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting up the models\n",
    "Both **GP Models** `gpytorch.models.ExactGP` with:\n",
    "- **Likelihood** `gpytorch.likelihoods.GaussianLikelihood`\n",
    "- **Zero Mean** `gpytorch.means.ZeroMean` for simplicity\n",
    "\n",
    "One model witch conventional kernel:\n",
    "- **RBF Kernel** `gpytorch.kernels.RBFKernel`\n",
    "\n",
    "One model with approximate kernel:\n",
    "- **Approximate RBF Kernel** `famgpytorch.kernels.RBFKernelApprox`"
   ],
   "id": "4860095475eda900"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T13:13:49.804103Z",
     "start_time": "2025-02-26T13:13:49.755966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConventionalGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_inputs, train_targets, likelihood):\n",
    "        super(ConventionalGPModel, self).__init__(train_inputs, train_targets, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ZeroMean(), num_tasks=2\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class ApproxGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_inputs, train_targets, likelihood):\n",
    "        super(ApproxGPModel, self).__init__(train_inputs, train_targets, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ZeroMean(), num_tasks=2\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            famgpytorch.kernels.RBFKernelApprox(), num_tasks=2, rank=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "conv_likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "conv_likelihood.to(device)\n",
    "conv_model = ConventionalGPModel(train_x, train_y, conv_likelihood)\n",
    "conv_model.to(device)\n",
    "\n",
    "approx_likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "approx_likelihood.to(device)\n",
    "approx_model = ApproxGPModel(train_x, train_y, approx_likelihood)\n",
    "approx_model.to(device)\n",
    "\n",
    "# manually initialize task kernel hyperparameters to make GPs comparable\n",
    "covar_factor = torch.randn(*conv_model.covar_module.task_covar_module.batch_shape, 2, 1, device=device)\n",
    "var = torch.randn(*conv_model.covar_module.task_covar_module.batch_shape, 2, device=device)\n",
    "hypers = {\n",
    "    \"covar_module.task_covar_module.covar_factor\": covar_factor,\n",
    "    \"covar_module.task_covar_module.raw_var\": var\n",
    "}\n",
    "conv_model.initialize(**hypers)\n",
    "approx_model.initialize(**hypers)\n",
    "None"
   ],
   "id": "d80e1f7949d8e2c6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the model",
   "id": "8c6466355560036f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T13:14:54.649940Z",
     "start_time": "2025-02-26T13:13:49.814702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set model in training mode\n",
    "conv_model.train()\n",
    "approx_model.train()\n",
    "conv_likelihood.train()\n",
    "approx_likelihood.train()\n",
    "\n",
    "# use adam optimizer, including the GaussianLikelihood parameters\n",
    "conv_optimizer = torch.optim.Adam(conv_model.parameters(), lr=0.1)\n",
    "approx_optimizer = torch.optim.Adam(approx_model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - marginal log likelihood\n",
    "conv_mll = gpytorch.mlls.ExactMarginalLogLikelihood(conv_likelihood, conv_model)\n",
    "approx_mll = gpytorch.mlls.ExactMarginalLogLikelihood(approx_likelihood, approx_model)\n",
    "\n",
    "conv_loss_item = None\n",
    "start = time.perf_counter()\n",
    "for i in range(1):\n",
    "    conv_optimizer.zero_grad()\n",
    "    conv_output = conv_model(train_x)\n",
    "    conv_loss = -conv_mll(conv_output, train_y)\n",
    "    conv_loss_item = conv_loss.item()\n",
    "    conv_loss.backward()\n",
    "    conv_optimizer.step()\n",
    "\n",
    "print(\n",
    "    f'Conventional:   Time: {time.perf_counter() - start:.3f} seconds   '\n",
    "    f'Loss: {conv_loss_item:.3f}   '\n",
    "    f'lengthscale: {conv_model.covar_module.data_covar_module.lengthscale.item():.3f}   '\n",
    "    f'noise: {conv_model.likelihood.noise.item():.3f}'\n",
    ")\n",
    "\n",
    "approx_loss_item = None\n",
    "start = time.perf_counter()\n",
    "for i in range(1):\n",
    "    approx_optimizer.zero_grad()\n",
    "    approx_output = approx_model(train_x)\n",
    "    approx_loss = -approx_mll(approx_output, train_y)\n",
    "    approx_loss_item = approx_loss.item()\n",
    "    approx_loss.backward()\n",
    "    approx_optimizer.step()\n",
    "\n",
    "print(\n",
    "    f'Approximate:    Time: {time.perf_counter() - start:.3f} seconds   '\n",
    "    f'Loss: {approx_loss_item:.3f}   '\n",
    "    f'lengthscale: {approx_model.covar_module.data_covar_module.lengthscale.item():.3f}   '\n",
    "    f'noise: {approx_model.likelihood.noise.item():.3f}   '\n",
    "    f'alpha: {approx_model.covar_module.data_covar_module.alpha.item():.3f}'\n",
    ")"
   ],
   "id": "6c4e11f178110a75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conventional:   Time: 32.130 seconds   Loss: 1.115   lengthscale: 0.644   noise: 0.644\n",
      "Approximate:    Time: 31.947 seconds   Loss: 1.115   lengthscale: 0.644   noise: 0.644   alpha: 0.744\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Resulting covariance matrix",
   "id": "f54208fa37862ff8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "with torch.no_grad():\n",
    "    conv_f_train = conv_model(train_x)\n",
    "    conv_train_covar = conv_f_train.covariance_matrix\n",
    "    print(\"\\tcovar:\\t\", conv_train_covar.__repr__().replace(\n",
    "        '\\n        ',\n",
    "        '\\n' + 5*'\\t' + ' '\n",
    "    ))\n",
    "\n",
    "    print(\"\\n--Approximate--\")\n",
    "    approx_f_train = approx_model(train_x)\n",
    "    approx_train_covar = approx_f_train.covariance_matrix\n",
    "    print(\"\\tcovar:\\t\", approx_train_covar.__repr__().replace(\n",
    "        '\\n        ',\n",
    "        '\\n' + 5*'\\t' + ' '\n",
    "    ))\n",
    "\n",
    "    rmse = torch.sqrt(torch.mean((conv_train_covar - approx_train_covar)**2))\n",
    "    print(f\"\\nRMSE: \", rmse)\n",
    "\n",
    "    # plot matrices\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "\n",
    "    vmin = torch.min(torch.cat((conv_train_covar, approx_train_covar))).item()\n",
    "    vmax = torch.max(torch.cat((conv_train_covar, approx_train_covar))).item()\n",
    "    # -- Conventional\n",
    "    im1 = ax1.imshow(conv_train_covar.to('cpu').numpy(), cmap=plt.colormaps['viridis'], vmin=vmin, vmax=vmax)\n",
    "    cbar = plt.colorbar(im1, ax=ax1)\n",
    "    ax1.set_title(\"Conventional\")\n",
    "    ax1.tick_params(left = False, right = False, labelleft = False, labelbottom = False, bottom = False)\n",
    "    ax1.grid(False)\n",
    "\n",
    "    # -- Approximate\n",
    "    im2 = ax2.imshow(approx_train_covar.to('cpu').numpy(), cmap=plt.colormaps['viridis'], vmin=vmin, vmax=vmax)\n",
    "    cbar = plt.colorbar(im2, ax=ax2)\n",
    "    ax2.set_title(\"Approximate\")\n",
    "    ax2.tick_params(left = False, right = False, labelleft = False, labelbottom = False, bottom = False)\n",
    "    ax2.grid(False)"
   ],
   "id": "d7d6fa3f05119be9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Make predictions with the models",
   "id": "43277807b94e9e5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Set into eval mode\n",
    "conv_model.eval()\n",
    "conv_likelihood.eval()\n",
    "approx_model.eval()\n",
    "approx_likelihood.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, nb_test_points, device=device)\n",
    "    conv_predictions = conv_likelihood(conv_model(test_x))\n",
    "    conv_mean = conv_predictions.mean\n",
    "    conv_lower, conv_upper = conv_predictions.confidence_region()\n",
    "\n",
    "    approx_predictions = approx_likelihood(approx_model(test_x))\n",
    "    approx_mean = approx_predictions.mean\n",
    "    approx_lower, approx_upper = approx_predictions.confidence_region()\n",
    "\n",
    "    # This contains predictions for both tasks, flattened out\n",
    "    # The first half of the predictions is for the first task\n",
    "    # The second half is for the second task\n",
    "\n",
    "# Initialize plots\n",
    "f = plt.figure(figsize=(8, 6))\n",
    "subfigs = f.subfigures(2, 1, hspace=0.06)\n",
    "subfigs[0].suptitle(\"Conventional\")\n",
    "subfigs[1].suptitle(\"Approximate\")\n",
    "for row, predictions in enumerate([conv_predictions, approx_predictions]):\n",
    "    mean = predictions.mean\n",
    "    lower, upper = predictions.confidence_region()\n",
    "    lower, upper = lower, upper\n",
    "\n",
    "    if device != 'cpu':\n",
    "        train_x = train_x.to('cpu')\n",
    "        train_y = train_y.to('cpu')\n",
    "        test_x = test_x.to('cpu')\n",
    "        mean = mean.to('cpu')\n",
    "        lower = lower.to('cpu')\n",
    "        upper = upper.to('cpu')\n",
    "\n",
    "    y1_ax, y2_ax = subfigs[row].subplots(1, 2)\n",
    "    # Plot training data as black stars\n",
    "    y1_ax.plot(train_x.detach().numpy(), train_y[:, 0].detach().numpy(), 'k*')\n",
    "    # Predictive mean as blue line\n",
    "    y1_ax.plot(test_x.numpy(), mean[:, 0].numpy(), 'b')\n",
    "    # Shade in confidence\n",
    "    y1_ax.fill_between(test_x.numpy(), lower[:, 0].numpy(), upper[:, 0].numpy(), alpha=0.5)\n",
    "    y1_ax.set_ylim([-3, 3])\n",
    "    y1_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    y1_ax.set_title('Observed Values (Likelihood)')\n",
    "\n",
    "    # Plot training data as black stars\n",
    "    y2_ax.plot(train_x.detach().numpy(), train_y[:, 1].detach().numpy(), 'k*')\n",
    "    # Predictive mean as blue line\n",
    "    y2_ax.plot(test_x.numpy(), mean[:, 1].numpy(), 'b')\n",
    "    # Shade in confidence\n",
    "    y2_ax.fill_between(test_x.numpy(), lower[:, 1].numpy(), upper[:, 1].numpy(), alpha=0.5)\n",
    "    y2_ax.set_ylim([-3, 3])\n",
    "    y2_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    y2_ax.set_title('Observed Values (Likelihood)')"
   ],
   "id": "69c71fc4ed1931ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
