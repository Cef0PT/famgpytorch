{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparing a conventional and approximate Multioutput-Output GP",
   "id": "d986c7538ae6165c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:59:58.865886Z",
     "start_time": "2025-02-19T19:59:57.440139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "import famgpytorch\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "torch.manual_seed(42)\n",
    "None"
   ],
   "id": "6499fd3423e574a7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set up some very simple training data\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_1 &= \\sin(2\\pi x) + \\epsilon \\\\\n",
    "y_2 &= \\cos(2\\pi x) + \\epsilon \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(0, 0.04)\n",
    "\\end{align}\n",
    "$$\n",
    "With training and test examples regularly spaced points in [0,1]"
   ],
   "id": "28601a995fe32cd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:59:58.910550Z",
     "start_time": "2025-02-19T19:59:58.868438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nb_training_points = 2\n",
    "nb_test_points = 1\n",
    "\n",
    "train_x = torch.linspace(0, 1, nb_training_points)\n",
    "\n",
    "train_y = torch.stack([\n",
    "    torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * math.sqrt(0.04),\n",
    "    torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * math.sqrt(0.04),\n",
    "], -1)"
   ],
   "id": "7d058d380727db1e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting up the models\n",
    "Both **GP Models** `gpytorch.models.ExactGP` with:\n",
    "- **Likelihood** `gpytorch.likelihoods.GaussianLikelihood`\n",
    "- **Zero Mean** `gpytorch.means.ZeroMean` for simplicity\n",
    "\n",
    "One model witch conventional kernel:\n",
    "- **RBF Kernel** `gpytorch.kernels.RBFKernel`\n",
    "\n",
    "One model with approximate kernel:\n",
    "- **Approximate RBF Kernel** `famgpytorch.kernels.RBFKernelApprox`\n",
    "\n",
    "Mean and Covariance modules are wrapped by the corresponding multitask module:\n",
    "- **MultitaskMean** `gpytorch.means.MultitaskMean`: one mean for each task\n",
    "- **MultitaskKernel** `gpytorch.kernels.MultitaskKernel`: Kernel supporting Kronecker style multitask Gaussian processes\n",
    "    - $K = K_{TT} \\otimes K_{XX}$"
   ],
   "id": "638c383ebfe760db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:59:59.028748Z",
     "start_time": "2025-02-19T19:59:58.991731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set up index kernel as task kernel to make gps comparable\n",
    "class ConventionalGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_inputs, train_targets, likelihood):\n",
    "        super(ConventionalGPModel, self).__init__(train_inputs, train_targets, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ZeroMean(), num_tasks=2\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class ApproxGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_inputs, train_targets, likelihood):\n",
    "        super(ApproxGPModel, self).__init__(train_inputs, train_targets, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=2\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            famgpytorch.kernels.RBFKernelApprox(), num_tasks=2, rank=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "conv_likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "conv_model = ConventionalGPModel(train_x, train_y, conv_likelihood)\n",
    "\n",
    "approx_likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "approx_model = ApproxGPModel(train_x, train_y, conv_likelihood)\n",
    "# approx_model.covar_module.data_covar_module.alpha = 2\n",
    "#approx_model.covar_module.data_covar_module.raw_alpha.requires_grad_(False)\n",
    "\n",
    "# manually initialize task kernel hyperparameters to make GPs comparable\n",
    "covar_factor = torch.randn(*conv_model.covar_module.task_covar_module.batch_shape, 2, 1)\n",
    "var = torch.randn(*conv_model.covar_module.task_covar_module.batch_shape, 2)\n",
    "hypers = {\n",
    "    \"covar_module.task_covar_module.covar_factor\": covar_factor,\n",
    "    \"covar_module.task_covar_module.raw_var\": var\n",
    "}\n",
    "conv_model.initialize(**hypers)\n",
    "approx_model.initialize(**hypers)\n",
    "None"
   ],
   "id": "89580b74a8551c9b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the model",
   "id": "6efefb2c02d17a49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:00:00.406897Z",
     "start_time": "2025-02-19T19:59:59.035265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set model in training mode\n",
    "conv_model.train()\n",
    "approx_model.train()\n",
    "conv_likelihood.train()\n",
    "approx_likelihood.train()\n",
    "\n",
    "# use adam optimizer, including the GaussianLikelihood parameters\n",
    "conv_optimizer = torch.optim.Adam(conv_model.parameters(), lr=0.1)\n",
    "approx_optimizer = torch.optim.Adam(approx_model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - marginal log likelihood\n",
    "conv_mll = gpytorch.mlls.ExactMarginalLogLikelihood(conv_likelihood, conv_model)\n",
    "approx_mll = gpytorch.mlls.ExactMarginalLogLikelihood(approx_likelihood, approx_model)\n",
    "\n",
    "for i in range(50):\n",
    "    # zero gradients from previous iteration\n",
    "    conv_optimizer.zero_grad()\n",
    "    approx_optimizer.zero_grad()\n",
    "\n",
    "    # output from model -> multivariate normal with mean vector and covariance matrix\n",
    "    conv_output = conv_model(train_x)\n",
    "    approx_output = approx_model(train_x)\n",
    "\n",
    "    # calc loss (negative marginal log likelihood)\n",
    "    conv_loss = -conv_mll(conv_output, train_y)\n",
    "    approx_loss = -approx_mll(approx_output, train_y)\n",
    "\n",
    "    # backprop gradients\n",
    "    conv_loss.backward()\n",
    "    approx_loss.backward()\n",
    "\n",
    "    if i == 0 or (i + 1) % 10 == 0:\n",
    "        print(f'Iter {i + 1:02d}/50')\n",
    "        print(\n",
    "            f'\\tConventional:   Loss: {conv_loss.item():.3f}   '\n",
    "            f'lengthscale: {conv_model.covar_module.data_covar_module.lengthscale.item():.3f}'\n",
    "        )\n",
    "        print(\n",
    "            f'\\tApproximate:   Loss: {approx_loss.item():.3f}   '\n",
    "            f'lengthscale: {approx_model.covar_module.data_covar_module.lengthscale.item():.3f}   '\n",
    "            f'alpha: {approx_model.covar_module.data_covar_module.alpha.item():.3f}'\n",
    "        )\n",
    "\n",
    "    # step on optimizer\n",
    "    conv_optimizer.step()\n",
    "    approx_optimizer.step()"
   ],
   "id": "177fd6756324e74b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 01/50\n",
      "\tConventional:   Loss: 1.600   lengthscale: 0.693\n",
      "\tApproximate:   Loss: 1.600   lengthscale: 0.693   alpha: 0.693\n",
      "Iter 10/50\n",
      "\tConventional:   Loss: 0.905   lengthscale: 1.248\n",
      "\tApproximate:   Loss: 1.275   lengthscale: 1.223   alpha: 0.343\n",
      "Iter 20/50\n",
      "\tConventional:   Loss: 0.282   lengthscale: 2.066\n",
      "\tApproximate:   Loss: 1.161   lengthscale: 1.718   alpha: 0.140\n",
      "Iter 30/50\n",
      "\tConventional:   Loss: -0.215   lengthscale: 3.016\n",
      "\tApproximate:   Loss: 1.121   lengthscale: 1.960   alpha: 0.050\n",
      "Iter 40/50\n",
      "\tConventional:   Loss: -0.571   lengthscale: 4.003\n",
      "\tApproximate:   Loss: 1.097   lengthscale: 2.029   alpha: 0.013\n",
      "Iter 50/50\n",
      "\tConventional:   Loss: -0.738   lengthscale: 4.906\n",
      "\tApproximate:   Loss: 1.088   lengthscale: 2.003   alpha: 0.003\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Resulting covariance matrix",
   "id": "88a06b0fb7252ba7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:00:00.473835Z",
     "start_time": "2025-02-19T20:00:00.414634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    print(\"--Conventional--\")\n",
    "    conv_f_train = conv_model(train_x)\n",
    "    conv_train_covar = conv_f_train.covariance_matrix\n",
    "    print(\"\\tcovar:\\t\", conv_train_covar.__repr__().replace(\n",
    "        '\\n        ',\n",
    "        '\\n' + 5*'\\t' + ' '\n",
    "    ))\n",
    "\n",
    "    print(\"\\n--Approximate--\")\n",
    "    approx_f_train = approx_model(train_x)\n",
    "    approx_train_covar = approx_f_train.covariance_matrix\n",
    "    print(\"\\tcovar:\\t\", approx_train_covar.__repr__().replace(\n",
    "        '\\n        ',\n",
    "        '\\n' + 5*'\\t' + ' '\n",
    "    ))\n",
    "\n",
    "    rmse = torch.sqrt(torch.mean((conv_train_covar - approx_train_covar)**2))\n",
    "    print(\"\\nRMSE:\", rmse)"
   ],
   "id": "f035d8af65cf88d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Conventional--\n",
      "\tcovar:\t tensor([[0.0022, 0.0108, 0.0021, 0.0106],\n",
      "\t\t\t\t\t [0.0108, 0.6989, 0.0106, 0.6850],\n",
      "\t\t\t\t\t [0.0021, 0.0106, 0.0022, 0.0108],\n",
      "\t\t\t\t\t [0.0106, 0.6850, 0.0108, 0.6989]], grad_fn=<MatmulBackward>)\n",
      "\n",
      "--Approximate--\n",
      "\tcovar:\t tensor([[0.0070, 0.0010, 0.0069, 0.0010],\n",
      "\t\t\t\t\t [0.0010, 0.0191, 0.0010, 0.0189],\n",
      "\t\t\t\t\t [0.0069, 0.0010, 0.0070, 0.0010],\n",
      "\t\t\t\t\t [0.0010, 0.0189, 0.0010, 0.0191]], grad_fn=<MatmulBackward>)\n",
      "\n",
      "RMSE: tensor(0.3366)\n",
      "\n",
      "--Conventional--\n",
      "\tcovar:\t tensor([[0.0022, 0.0108, 0.0021, 0.0106],\n",
      "\t\t\t\t\t [0.0108, 0.6989, 0.0106, 0.6850],\n",
      "\t\t\t\t\t [0.0021, 0.0106, 0.0022, 0.0108],\n",
      "\t\t\t\t\t [0.0106, 0.6850, 0.0108, 0.6989]], grad_fn=<MatmulBackward>)\n",
      "\n",
      "--Approximate--\n",
      "\tcovar:\t tensor([[0.0070, 0.0010, 0.0069, 0.0010],\n",
      "\t\t\t\t\t [0.0010, 0.0191, 0.0010, 0.0189],\n",
      "\t\t\t\t\t [0.0069, 0.0010, 0.0070, 0.0010],\n",
      "\t\t\t\t\t [0.0010, 0.0189, 0.0010, 0.0191]], grad_fn=<MatmulBackward>)\n",
      "\n",
      "RMSE: tensor(0.3366)\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
